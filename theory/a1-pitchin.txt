Thank you for the briefing. Now, I can understand more about the position.
After listening to the conversation, I am more confident to say that, “I am best fit for this role”

INTRODUCTION:
-------------
I am a Full-Stack Python Developer, with all years of experience, with around 80% into Backend work and 20% into the frontend.

IN Backend, I worked on creating standalone scripts for automation, scheduled jobs, ETL jobs and data pipelines; and RESTful APIs and web applications.

IN FRONTEND, I worked on creating dashboards/graphs/charts using d3.js or high charts libraries, and tables using ag-grid. Also, worked on JavaScript, jQuery, and react.

In databases,
- In relational dB, I worked with MySQL, MS SQL, oracle dB and PostgreSQL.
- In no-relational dB, I worked with MongoDB and Cassandra.
- In data warehousing, I worked with Snowflake schema

In Python,
- worked on creating web applications and RESTFUL APIs using Django/flask/FastAPI frameworks
- Also, worked on creating process automation and integrating with infrastructure(Linux/windows) boxes.
- Apart from these, I also worked on data gathering from
	1) structured datasources like REST APIs (internal or external), databases, etc.
	2) unstructured datasources like web scraping using beautiful soup
	3) Also, worked on structured, semi-structured or unstructured file types like csv, excel, json, yaml, parquet, etc.
- As part of projects, I followed TDD (test driven development).
	So, created unit tests and integration tests for the source code, using either unittest or pytest modules.

In Caching,
- I worked with Redis and Memcache

In scheduling,
- I worked with celery.
- I did integrations with django applications for periodic jobs

For datajobs orchestration, I worked with Airflow

In Public Cloud, I am mostly associated with AWS Cloud.
In AWS Cloud, I worked with
	Server-based Architectures with
		- EC2 Instances
		- Elastic BeanStack
		- Elastic Load balancer
		- Auto-Scaling
		- Route53

	for Storage,
		- s3 bucket for file storage
		- s3 Glacier for Archival Storage

	In Databases,
		AWS RDS & Redshift for relational databases
		AWS DynamoDB for nosql

	Server-less Architectures with
		- AWS Lambda - with either time-triggered or event triggered
		- AWS API Gateway with http-triggered

		AWS Event Scheduler for scheduling the lambda

	For oauth, I worked with aws cognito
	- Also, worked with SQS, SNS and SES
	- And, EMR cluster for Pyspark
	- for the ETLs, I worked with AWS Glue Jobs with DataCatalog and Pyspark

Also, have experience in creating the CI/CD setup using Jenkins and Groovy script.

In terms of Infrastructure as a code, Worked mainly with Cloudformation templates, then terraform and Pulumi python module too

And, I have experience in agile methodologies like Scrum, Kanban in facilitating agile ceremonies like daily standups,sprint reviews, plannings and retrospectives. I am familiar with Agile tools like Jira and skilled in working with cross-functional teams including Dev team, QA testers, and PM/PO's.

In GCP, worked with
	VMs for server
	GCP functions for Serverless
	GKE and CloudRun for Container orchestration
	CloudSQl with PostgreSQL

In Azure Cloud,
	Server-based Architectures:
		Azure Virtual Machines, Azure App Service,  Azure Load Balancer,  Azure Virtual Machine Scale Sets, Azure DNS
	Storage:
		Azure Blob Storage, Azure Blob Storage Archive Tier
	Databases:
		Azure SQL Database & Azure Synapse Analytics,  Azure Cosmos DB
	Serverless Architectures:
		Azure Functions,  Azure API Management + Functions,  Azure Logic Apps, Azure Active Directory
	Messaging:
		Azure Queue Storage, Azure Service Bus, Azure SendGrid
	Big Data:
		Azure HDInsight,  Azure Data Factory, Azure Data Catalog

I have experience in the entire data wrangling activities, from data source, transformations, cleansing and finally creating golden records

In terms of Data Visualization, I worked with
	Python modules like matplotlib, seaborn, plotly
	Javascript libraries like d3.js and highcharts

        PowerBI and Tableau
Created charts, graphs, pie/ sunburn/ and interactive charts


In Golang, I created microservices and ETL jobs,
	Mainly using go concurrency.
	Used Gokit for microservices implementation,
	and Cobra package for creating command line utilities.
	GORM - for working with the databases
	BDD with ginko framework
	Learning the GRPC

REASON FOR CHANGE
=================
	CONTRACT COMPLETED. NO more SCOPE FOR EXTENSION.
	SO, Looking for a change.
	But It is a very interesting project.

	Recently, our company made a policy change to make work from office mandatory.
	It is becoming difficult to commute daily to office, and manage my recently born baby boy.
	Also, my spouse is worried about any potential covid outbreak again.
	So, I am looking for remote role, as I did the same from outbreak of COVID onwards.

Rate yourself
=============
	For Python, I would say 9/10
	For SQL, I would count as 8/10, I didnt work in managing databases, only as a developer
			I did create db design, complex queries, joins, sql procs/functions/db triggers, etc

how do you learn new technologies, and trends
=============================================
	- I subscribed to various blogs and weekly newsletters
	- Apart from them, I go through the stackoverflow questions and workplace stackoverflow questions, in my free time.
	- And, part of enhancements or POCs, I research and read, and sometimes implement, based on some articles
	- Sometimes, I will listen to some tech related podcasts.
	- Most important , is it will practice
		- Complete the exercises

Python Podcasts
================
	- TalkPython.fm

Python Books
=============
	Learning Python by Mark Lutz
	Python Distilled by David Beazley

People in Python community
==========================
	- Sam Lau and Philip Guo, who built the pythontutor and pandastutor websites

Learning new language
======================
	I'm very much interested in learning a new language.
	One receent example I would like to quote is
	There is an old legacy application, from which we were given access to get information, within a specific window of time.
	So, I tried using python muti-threading/multi-processing combination, but I wasnt satisfied with the outcome.
	When researched, I found that Go language is better suitable for concurrent microservice calls, and worked on it.

	With this I want to say like you I'm pretty much flexible enough to learn a new language and adapt.

When Programming started
========================
	I am interested in playing online games.
	When I was in my graduation, I hope you remember, angry birds became a big hit.

	I played different games, but this is simple.
	So, I thought of creating one.
	When researched, I found some code online.
	I just created a clone for it. Added some changes, for fun.
	I worked with pygame, which is a python module.

	Then, when looking for job, as I have some knowledge in python,
	I looked for work in it, and landed as a python developer.
	That is my journey

Documentation Experience
------------------------
	I have experience in writing both high-level documentation (using Atlassian Wiki) and Low-level documentations using docstrings in code, and using pydoc/ sphinx modules

-------------------------
Questions to Interviewers
-------------------------
	- May I know the Business problem that we are about to address with this project?
	- May I know about the project, and the system architecture like the tools used, database, etc?

	- What is the volume of data, which we will be working on, on a daily basis? Some approximate numbers of records?
	- What kind of data do we work with, and what are our data sources?
	- Were we using any custom/in-house frameworks/tools in our project? If so, can you help with the details? (like your own databases, etc..)

	- What are you expecting from me, for this role? What can be the day-to-day activities? what does a successful person look like on your team?
	- within the first 3-4 months of hire, what are the pain-points you would like me to tackle?
	- In the next 3-6 months, what were your major priorities, towards this project?

	- Hope we are using agile methodology!  And how about the sprint cycle duration?
	- Can I know about the work culture of our company?

	- What were the next steps in the interview process? And how many more rounds of interviews can I expect?

	CONCLUSION:
		Thank you for the time, and I want to conclude that I am an expert in Python.
		I have experience working in both Green-field and Blue-Green Developments.
		Experience in both backend and frontend, data wrangling and ETLs
		And I am ready to extend and learn new skills, if needed.

		Have experience in working on financial products, best practices to follow. So, I can be productive, with minimal training.


	- As I'm understanding the current landscape of your environment, what is your vision of future innovation and or new technologies you want to bring within the next 2-4 years, and why so?

	- Will there be an opportunity to reach out to the clients, either for requirement gathering and/or for demoing the end solution?
	- Have I answered all your questions?
	- Do you have any hesitations about my qualifications?
	- What is the biggest challenge your company is facing today?
	- What do you enjoy most about working in this company?
	- Whats are the characteristics of someone who would succeed in this role?
	- What metrics or goals will my performance be evaluated against?
	- What are the most immediate projects that need to be addressed?
	- Can you describe a typical day or week in the job?
	- What will be the process of getting up to speed?
	  In our team, we use confluence and teams' channels to collaborate? How do they collaborate and maintain documentation?
	- What will be the area that you want this role to be focusses?
	- Do you expect the main responsibilities for this position to change in the next six months to a year?
	- What are the current goals that the company is focussed on, and how does this team work to support hitting those goals?
	- Who will I report to directly?
	- What are the common career paths in this department?
	- Is there anything else I can provide you with that would be helpful?
	- What's one of the most interesting projects or opportunities that you've worked on?

company expenses
-----------------
	Green Dollar Expense -- busines from external vendors like AWS Cloud
	Blue Dollar Expense  -- business between LOBs, in same organization
================================================================================
We have Built micro services for different use cases with Rest APIs using aws api gateway and Lambda and
integrated these APIs with several components to get, post, delete info from the database.

Created IAM role based services that can access aws s3, dynamodb for data storage.

Worked on aws EMR clusters with python, spark to move large data from different sources, transform and
load into mongodb and cloud databases like auroradb/datalake s3.

Used Django and python to build an application which shows the data movement statistics and related graphs.

hands-on experience using Python in developing data-based applications,


Reading the unstructured large text files (around 70 Gigabytes each from external vendor) into Pyspark DataFrames, cleaning the data and making it structured and storing it in Amazon S3 to use it for further processes.

Worked on high configuration EMR cluster to run Spark jobs and created bootstrap scripts to install additional applications required on the EMR cluster.

Based on the new or updated business requirements, redesigned and implemented the Rules for Processing Workflows.

Analyzing existing applications consisting of multiple batch programs in SAS and developed the code in Python and Spark.

Built new connections to databases like ORE, Snowflake, Redshift, and Teradata from spark and python and
incorporated everything in a python library to make the DA's life easier.

Rewrote the Teradata SQL queries to Amazon Snowflake as part of database migration.
========================================================================================================
I have 6+ years of experience in the IT industry as a Python developer.
Currently working with Capital one we mainly focus on providing a better customer experience.
I played the role of both a python developer & frontend developer.

Hands-on experience in Python and front-end technologies, HTML, CSS, JS, Angular, Bootstrap, React Js,
Node.js and AWS, databases like PostgreSQL, Teradata, MySQL, Dynamo DB


My responsibilities included building key services such as data scraping from various sources and
creating a browser extension using python. I also make sure to work in a TDD environment, where I
write test cases for each service I developed.

In terms of APIs, I was involved in building multiple endpoints to scale our application and we
used AWS API Gateway to handle API requests and responses. To analyze our user data, I used the
pandas and NumPy libraries, and we used AWS Glue to extract data from our S3 data lake and create metadata.

Additionally, we used AWS Quick Sight for real-time metrics and data visualization.
All of this allowed us to quickly analyze customer data and provide relevant recommendations to our credit card holders.

================================================================================
Intro
I am experienced python developer with  years of hands-on experience in the field. I believe I have a deep understanding of Python's frameworks, its libraries , algorithms, and object oriented programming. I am well versed with developing web based applications using python, CSS, HTML, Json, and Javascript. My primary Tech stack is Python, PySpark, SQL, AWS and Airflow.

I am also having good knowledge in writing subqueries, Stored Procedures, Triggers, cursors, and functions in mySQL and PostgreSQL database. I worked on Python ORM libraries including Django ORM, SQL Alchemy. I am comfortable with pythons IDE's like pycharm, Pystudio, Pydev, Pycharm, Sublime text and I have good knowledge on Jupyter notebook. I worked on building frameworks and automating workflows using python for automation testing. I have good knowledge in amazon web service concept like EMR and EC2 web services for processing big data. I have worked with docker for deploying the application inside software containers. I have experience in Microsoft data storage and Azure Data factory, Azure Data Lake Store (ADLS), AWS S#, EC2, Vault, Lambda, SQS and RDS.

I have worked with python development using Numpy and Pandas in Python for data manipulation. I also have some hands on experience on some other libraries like matplotlib for charts and graphs, MySQLdb for database connectivity, Python-twitter, Pyside, Pickle, Panda's data frame, network, urllib2.

I have worked on migrating on-prem ETLs from MS SQL server to Azure CLoud using Azure Data factory and Databricks. I am familiar with machine learning concepts like NLTK , OpenNLP and StanfordNLP for natural language processing and sentiment analysis. I am familiar with working on Unix and Linux Shell environments using command line. I also developed web applications using XML, Json and XSL for front end development.

I am passionate about coding and enjoy keeping up-to-date with the latest technologies and trends in the industry. Believe my skills and experience make me a valuable addition to any team, and I look forward to sharing my knowledge and contributing to the success of your project.
================================================================================
Data Engineering
---------------
I worked on
	Data Gathering - using tools, and using python
		Worked on both structured,  unstructured and semi-structured data

		Worked with Both REST APIs and GraphQLs

	Data Cleansing - worked with regular expressions
	Data Validations, using greatexpectations
	Data Transformations - Worked with Python Pandas

	Data Visualization and Reporting -- with PowerBI and Python based modules

Expert in SQL too

worked with files
	tex, csv, parquet, Avro

In databases,
	- In relational dB, I worked with MySQL, MS SQL, oracle dB and PostgreSQL.
	- In no-relational dB, I worked with MongoDB and Cassandra.
	- In data warehousing, I worked with Snowflake schema


1. -- data gather
2. -- cleansing
3. -- transformation
4. -- dumping to db
5. -- creating REST API
6. -- finall consuming from powerBI
