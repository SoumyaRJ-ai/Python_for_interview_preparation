Current Project is into data analytics
We have a couple of data scientists, who will drive this data analytics process.

my responsibilities here include
	- data gathering from both internal and external data sources
		- including REST APIs, SOAP interfaces, GRAPHQL, and internal databases
	- also, writing scripts for ETL jobs, data transformations, and for the business logic
	- finally, the cleansed and transformed data will be exposed as an API, for both UI consumption and for app to app communication

	70% time for coding
	10% time for testing
	10% time for code review
	Remaining – client and Stakeholder discussions, agile ceremonies

	When we started this project, we were just two members in the team.
	I am part of initial design discussions, dataflow design, db design, creating JIRAs, implementing, demoing the clients, and finally shipping of product.

	We initially started on small POC, after approvals got budgets for team expansion, and created an MVP product, and then stable releases.

	Docker.yaml → Jenkins CI/CD → containers  -> AWS ECR →  aws code pipeline → AWS EKS → kubernetes.yaml → orchestrate the pods

In terms of team,
	In backend, apart from me, there were two junior developers
	In frontend, two developers were present
	And we have a couple of data scientists, who will architect the dataflows

In Machine Learning,
	I worked with a Linear regression algorithm, using the scikit-learn module.
	The use-case is that there was an archive database.
	We need to do future predictions based on these past values.

	I created a model and also exposed it via a flask application, for further training.

In terms of technical stack,
	Our application server is in an EC2 instance, and celery workers are running on another server.
	Both are in multiple regions for DR (disaster recovery) strategy.

	Which is behind the autoscaling and inturn, behind the Route53

	All my data feeds and ETL jobs are microservices written in either aws lambda,
	or aws step functions; or recently, with container based microservices
	Used docker for containerization and kubernetes for orchestration.

    Project Stack
    --------------
    Frontend    : Django Template Language(DTL), Jinja, Javascript, jQuery, D3.js, Aggrid
    Backend     : Django, Django Rest Framework (DRF)
    Cache       : Redis
    Queuing     : RabbitMQ
    Scheduler   : Celery

	Application server(s) (Gunicorn) fronted by Nginx
	Worker server(s) (Celery)
	Scheduler server (Celery beat + shell, single instance)
	Database (Postgres)
	Cache (Redis usually, sometimes Memcache)
	Message Server for Celery (Redis or RabbitMQ)
	Static content pushed to and served from S3
	A CDN in front of the static content on S3


Project - CASH app storing ATM informations
============================================
	The application that I have worked on is the CASH App application where it stores the ATM Schedule information. so what happens is all the
	service schedules related to the ATM is stored in this application. All the permutations and the combintions of ATM schedules are stored in the database related to the application.

	All the ATM Schedules information that is stored in db has to be sent to the vendors very morning. In order to do that I have set up a cron job in the ec2 instance in aws cloud platform
	that would send out the files to the external file gateway. I have successfully onboarded the application to onepipeine.We are using RDS Database and the application has to run on that.

	I have completed doing the RDS Instance setup in dev, UAT and in production as well. I have Modified the existing code to the 3.9 version of python and I have Used flak framework to develop that application. I am responsible for Cleaning up the existing code and making it more efficient for the users to understand and i am also responsible for the modularization of the code.

	The code for the whole application is divided into the modules and the supporting modules for this application are maintained under one directory. each module hold separate functionality to maintain easily. I have written All the unit test cases and maintained in the tests directory. Each test modules is created & maintained for each module in the main application.

	In this project, We are using AWS Cloud and in this cloud particularly we are working with AWS API's lambda functionalities and with s3, buckets, and with CloudWatch for simple notification services, EC2, ECS, ECR and like RDS, and IAM security groups and all these things etc.

	Coming to the front end part of the application , I've worked with HTML, CSS, JavaScript bootstrap, Angular, and testing frameworks like jasmine and karma,
	and also have upskilled with React and I am comfortable with the React , but my projects were more on the angular side.

	So, then like I've used tools like sonar lint, sonar cube for code quality checks.

	I have used postman as the API testing tool before integrating with the front end to simplify and streamline the process of developing, testing,  and for sending HTTP requests to APIs and receiving responses, making it easier to interact with APIs during development and testing phases and also used jenkins build pipelines for the automation and continouous integration as well.

	I have also worked on writing the python scripts that automates the holiday schedules for the capital one in order to send the data to the venors as well as worked on the python script can generate reports by pulling data from various sources, processing it, and presenting it in a formatted document, such as PDF or Excel and I am using the Python's data processing libraries, such as Pandas and NumPy, for the collected data in order to get transformed and aggregated.

	This is pretty much I have worked in my current project.

------------------------------------------------------
In AWS cloud, my roles involve,
	I will create the architecture,
	We will implement using the cloud formation templates
	And integrate with ci/cd

	We will do POCs with new components in aws, and if success, we try to incorporate in architecture,
	using cloud formation templates

Day-to-day activities
--------------------
- on a daily basis,
	Initially when I start I will check for any emails or alerts which we need to address.
	So, and if I need to reach out any stakeholders I will reach out to them,
	and then like I started working on my JIRAs And after the stand up during the stand up like you know, so
	we discuss, what are the pain points or any you know blockers.
	Then finally, so after the, you know, the standup like I will start working on my JIRAs.
- We prioritize based on the situation and sometimes, i will take manager’s decision

---------------------------------------------------------------------------------
In terms of the technical stack,
	we have a web application written in Django, with frontend as React
	This web application is a monolithic architecture

	For the data collections, transformations and business logic,
	I worked on creating multiple microservices with AWS Lambda
		I created data pipelines with s3 bucket to aws lambda with time and event triggers
		Also, exposed lambdas with API Gateway for other teams, for data infusion to my db

Current Project - Fault-analysis for medical devices @ Medtronic
-----------------------------------------------------------------
	- We are into medical devices manufacturing and servicing.
	- Medtronic's implantable devices are supported by an ecosystem of external-to-the-body devices,
	  such as remote monitors and services that include programmers attached to HCPs(healthcare Providers) and hospital locations.
	- we collect the anonymous data from these medical devices, and store in mongodb
	- we do post-analysis on this archival data.


Current Project - Bank of America - Financial transactions analysis
-----------------------------------------------------------------------
	 - WallMart can store all the financial transactions
	 - But for small organization, for capturing their transactions for post analysis
	 - we are here for that transaction capturing.


Current Project - T-Mobile
-------------------------------
	- I am part of a team which will create cellular propagation models.
	- Means for cellular call drop analysis


Current Project - Fault-analysis project
------------------------------------------
	- At Denso, we sell vehicle spare parts

	we sell vehicle spare parts, and some of the products will send back some
	anonymous data back to our servers.


Current Project - Customer Behaviour Analytics project
------------------------------------------
	- I am part of the customer behaviour analytics team.
	- If you come to our website, you will notice some cookies.
	- These cookies will send the user-analytics data feeds, back to
	  our databases.
	- We analyse those offline data, and bring insights from them.
	- These results will be used in different domains, within the organization.


Current Project - Customer Feedback messages Analyses
-----------------------------------------------------
	current project details with Customer feedback analysis.

	Objective of the project is to analyse the customer feedback messages,
	classify and summaries the problems, and report to the concerned department.

	My responsibilities include
		- Creating data pipelines for data tranformations of raw messages,
		- then, created ML models and trained with this data.
		- Finally, the prediction results were stored in database.

		- Also, create Django web application, to dashboard the results and REST APIs for app-to-app communication.


Current project - Mortgage analytics
-----------------------------------
	- we have mortgage products.
	- I am here, part of the R&D team, doing research on these products and their
	  enhancements.

Current project - RealEstate
------------------------------
	- CBRE works in Real-Estate market
	- I am part of the planning department.
	- Our team is responsible for giving the prediction, for better planning
	- We collect data from different sources like climate info, crop yield, agriculture growth, geographic and geological informations

	As part of CBRE's real estate planning team, we rely heavily on making accurate predictions to advise optimal locations and types of properties to develop.

	Recently, we started collecting large amounts of unstructured data from surveys, news articles, social media posts related to real estate market conditions.

	To make use of this valuable data, I decided to employ natural language processing techniques in Python to extract insights.

	First, I collected the textual data into a corpus and preprocessed it by cleaning, normalizing and filtering out noise.

	Next, I performed topic modeling using latent semantic analysis to identify key themes and trends in the corpus. This gave us useful topic clusters related to real estate like "housing demand", "rental prices", "new construction" etc.

	Using named entity recognition, I could extract useful entity types like locations, property types, company names which provided geographical and market insights.

	For sentiment analysis, I built a classifier model using Python's NLTK library to categorize text snippets as positive, negative or neutral sentiment. This helped gauge public opinion on locations and projects.

	By analyzing sentiment trends over time, we could correlate it with actual real estate price movements to include sentiment as a variable in our pricing prediction models.

	Using part-of-speech tagging and parsing, I could extract useful statistics like percentages of adjectives used or frequencies of key phrases. These text metrics offered additional predictive signals.

	In the end, combining natural language processing with our structured data led to more accurate real estate forecasting and recommendation models using Python. This improved our planning and business outcomes significantly.

Current Project - Products Feedback Analytics
------------------------------------
	- We are part of the R&D team.
	- We have access to the data lake, which has all the meta data of the products and their events.

	- Objective of this project is
		- We will group the problems and cluster them and associate them to the products.
		- This output will help in next versions
	- Stakeholders are internal product Owners of this

Current Project - MetaData Analytics
------------------------------------
	I am part of Metadata analytics project.
		This project is for collection and storage of metadata of all datasets.
		This metadata is used for understanding the business Gap Analytics and other anomaly detections.

	This Project does two things
		1) Collection metadata, about all the registered datasets of internal applications
			With that info, help them streamline their data, Identify anomalies, outliers, etc.
		2) Based on that metadata, once stabilised, We will create synthetic data, which is input for the ML models, within the Organization

	I was part of this project from the ideation stage, to design, implementation setup,
		and demoing to the clients and post support.
		Majorly I contributed in writing python scripts
			- for the metadata analysis,
			- for creating the ETLs as part of the business requirements
			- REST APIs and web application for the application data consumption
		Also, I have first hand experience in requirement gathering, refinements, creating Jira, and demoing to clients, and getting feedback.

Current Project - Retail Web application
-----------------------------
	The project involved building a web application that allowed customers to browse through the company's product catalogue, place orders, and make payments. Additionally, the application provided an administrative interface for the company's staff to manage product inventory, process orders, and generate reports.

	To build the application, I used the Django web framework, which allowed me to quick prototype and develop the application's core features. I also used various third-party packages, such as stripe for payment processing and celery for background task processing.

	The application had several key features, including a search function that allowed customers to search for produycst based on keywords or categories. The application also had a recommendation engine that suggested similar products to customers based on their browsing and purchase history.

	One of the challenges of the project was optimizing the application's performance to handle high traffic during peak periods. To achieve this, I implemented caching mechanisms, optimized database queries, and used load balancing techniques.

	Another important aspect of the project was ensuring the security of the application. I implemented various security measures, such as data encryption, secure login authentication, and input validation to prevent SQL injection and other types of attacks.

	Overall, the project was a success, and the company was able to increase its online sales and improve customer satisfaction. It was a great opportunity to apply my python development skills to a real-world business problem and make a tangible impact.

Current Project - Healthcare
-----------------------------


Current Project - Infra Automation - 1 click automation -- AWS CLOUD automation
---------------------------------------------
	- project is boto3 Tagging based Resource billing and Analytics

	- We are part of the infrastructure management team, within this company.
	- Our job is to manage the infrastructure within the organization.
	- we will grant access to cloud resources to the teams, and charge for the same, from their projects.

	- In the process, to optimise the billing and better manage, we need to uniquely recognize the resources.
	- For that, I wrote boto 3 scripts to scan all existing resources and add tags to them.
	- Similarly, based on Event hub, automatically added tags for newly creating resources too.
	- We have different types of tags, like user, team, etc, to different kinds of analytics
 	- Now, based on these tags, did different sets of analytics and give insights based on them, like
		- billing
		- cost optimization
		- rule/access violations and/or potential violations
		- security audits, etc

Current Project - Data Engineering
----------------------------------
	- It is to build a data pipeline.
	- The data source is the aws s3.
	- The spark inside the aws databricks extracts the data from aws s3.
	- Then, creates the dataframe and does the actual data cleansing, tranformations, deduplication.
	- Then, the final data will be pushed to aws Snowflake datawarehouse.

	- The primary tech stack is : Python, Pyspark, SQL, AWS, Airflow

	- I have experience on building the both batch and streaming the data pipelines.
	- I am running spark job in EMR cluster scheduled through oozie scheduler . Basically this job is reading csv files based on file schema.
	- Creating shell scripts to process the raw data, loading data to AWS S3 and Redshift databases.
	- Also wrote spark scala API’s. Automating the jobs in Hadoop and Linux using Autosys tool.

	- Sqooping data from relational databases like Oracle to hdfs/hive and vice versa.
	- experience with hadoop ecosystem and bigdata related distributories.
	- Used Hadoop tools like
		- hdfs for storage,
		- mapreduce to process data in backend,
		- hadoop data warehouse as hive, in order to export and import data from different rdbms written the sqoop scripts.

project documentation
=====================
- There are two types of documenatation, I do.
1) Active documentation
	- It is shared with all stack holders.
	- It is needed as part of the process, for the code review, and further, for signoff, from stackholders, for code production release.
2) Pasive documentation
	- I create for project source code maintenance.
	- I write both comments and docstrings.
	- Use pydoc module, for creating documentation from these docstrings


TODO - Steganography with python
