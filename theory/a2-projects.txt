Current Project is into data analytics
We have a couple of data scientists, who will drive this data analytics process.

my responsibilities here include
	- data gathering from both internal and external data sources
		- including REST APIs, SOAP interfaces, GRAPHQL, and internal databases
	- also, writing scripts for ETL jobs, data transformations, and for the business logic
	- finally, the cleansed and transformed data will be exposed as an API, for both UI consumption and for app to app communication

	50% time for coding
	10% time for testing
	10% time for code review
	Remaining – client and Stakeholder discussions, agile ceremonies

	When we started this project, we were just two members in the team.
	I am part of initial design discussions, dataflow design, db design, creating JIRAs, implementing, demoing the clients, and finally shipping of product.

	We initially started on small POC, after approvals got budgets for team expansion, and created an MVP product, and then stable releases.

	Docker.yaml → Jenkins CI/CD → containers  -> AWs ECR →  aws code pipeline → AWS EKS → kubernetes.yaml → orchestrate the pods

In Machine Learning,
	I worked with a Linear regression algorithm, using the scikit-learn module.
	The use-case is that there was an archive database.
	We need to do future predictions based on these past values.

	I created a model and also exposed it via a flask application, for further training.

In terms of technical stack,
	Our application server is in a EC2 instance, and celery workers are running on another server.
	Both are in multiple regions for DR (disaster recovery) strategy.

	Which is behind the autoscaling and inturn, behind the Route53

	All my data feeds and ETL jobs are microservices written in either aws lambda,
	or aws step functions; or recently, with container based microservices
	Used docker for containerization and kubernetes for orchestration.

    Project Stack
    --------------
    Frontend    : Django Template Language(DTL), Jinja, Javascript, jQuery, D3.js, Aggrid
    Backend     : Django, Django Rest Framework (DRF)
    Cache       : Redis
    Queuing     : RabbitMQ
    Scheduler   : Celery

	Application server(s) (Gunicorn) fronted by Nginx
	Worker server(s) (Celery)
	Scheduler server (Celery beat + shell, single instance)
	Database (Postgres)
	Cache (Redis usually, sometimes Memcache)
	Message Server for Celery (Redis or RabbitMQ)
	Static content pushed to and served from S3
	A CDN in front of the static content on S3

In terms of team,
	In backend, apart from me, there were two junior developers
	In frontend, two developers were present
	And we have a couple of data scientists, who will architect the dataflows
------------------------------------------------------
In AWS cloud, my roles involve,
	I will create the architecture,
	We will implement using the cloud formation templates
	And integrate with ci/cd

	We will do POCs with new components in aws, and if success, we try to incorporate in architecture,
	using cloud formation templates

Day-to-day activities
--------------------
- on a daily basis,
	Initially when I start I will check for any emails or alerts which we need to address.
	So, and if I need to reach out any stakeholders I will reach out to them,
	and then like I started working on my JIRAs And after the stand up during the stand up like you know, so
	we discuss, what are the pain points or any you know blockers.
	Then finally, so after the, you know, the standup like I will start working on my JIRAs.
- We prioritize based on the situation and sometimes, i will take manager’s decision

----------------------------------------------------------------------------------------
In terms of the technical stack,
	we have a web application written in Django, with frontend as React
	This web application is a monolithic architecture

	For the data collections, transformations and business logic,
	I worked on creating multiple microservices with AWS Lambda
		I created data pipelines with s3 bucket to aws lambda with time and event triggers
		Also, exposed lambdas with API Gateway for other teams, for data infusion to my db

Current Project - Fault-analysis for medical devices @ Medtronic
-----------------------------------------------------------------
	- We are into medical devices manufacturing and servicing
	- we collect the anonymous data from medical devices, and store in mongodb
	- we do post-analysis on this archival data.


Current Project - Bank of America - Financial transactions analysis
-----------------------------------------------------------------------
	 - WallMart can store all the financial transactions
	 - But for small organization, for capturing their transactions for post analysis
	 - we are here for that transaction capturing.


Current Project - T-Mobile
-------------------------------
	- I am part of a team which will create cellular propagation models.
	- Means for cellular call drop analysis


Current Project - Fault-analysis project
------------------------------------------
	- At Denso, we sell vehicle spare parts

	we sell vehicle spare parts, and some of the products will send back some
	anonymous data back to our servers.


Current Project - Customer Behaviour Analytics project
------------------------------------------
	- I am part of the customer behaviour analytics team.
	- If you come to our website, you will notice some cookies.
	- These cookies will send the user-analytics data feeds, back to
	  our databases.
	- We analyse those offline data, and bring insights from them.
	- These results will be used in different domains, within the organization.


Current project - Mortgage analytics
-----------------------------------
	- we have mortgage products.
	- I am here, part of the R&D team, doing research on these products and their
	  enhancements.

Current project - RealEstate
------------------------------
	- CBRE works in Real-Estate market
	- I am part of the planning department.
	- Our team is responsible for giving the prediction, for better planning
	- We collect data from different sources like climate info, crop yield, agriculture growth, geographic and geological informations

Current Project - MetaData Analytics
------------------------------------
	- We are part of the R&D team.
	- We have access to the data lake, which has all the meta data of the products and their events.

	- Objective of this project is
		- We will group the problems and give
		- This output will help in next versions

	- Stakeholders are internal product	Owners of this

Current Project - Infra Automation - 1 click automation -- AWS CLOUD automation
---------------------------------------------
	- project is boto3 Tagging based Resource billing and Analytics

	- We are part of the infrastructure management team, within this company.
	- Our job is to managem the infrastructure, within the organization.
	- we will grant access to cloud resources to the teams, and charge for the same, from their projects.

	- In the process, to optimise the billing and better manage, we need to uniquely recognize the resources.
	- For that, I wrote boto 3 scripts to scan all existing resources and add tags to them.
	- Similarly, based on Event hub, automatically added tags for newly creating resources too.
	- We have different types of tags, like user, team, etc, to different kinds of analytics
 	- Now, based on these tags, do different sets of analytics and give insights based on them, like
		- billing
		- cost optimization
		- rule/access violations and/or potential violations
		- security audits, etc

Current Project - Data Engineering
----------------------------------
	- It is to build a data pipeline.
	- The data source is the aws s3.
	- The spark inside the aws databricks extracts the data from aws s3.
	- Then, creates the dataframe and does the actual data cleansing, tranformations, deduplication.
	- Then, the final data will be pushed to aws Snowflake datawarehouse.

	- The primary tech stack is : Python, Pyspark, SQL, AWS, Airflow

	- I have experience on building the both batch and streaming the data pipelines.
	- I am running spark job in EMR cluster scheduled through oozie scheduler . Basically this job is reading csv files based on file schema.
	- Creating shell scripts to process the raw data, loading data to AWS S3 and Redshift databases.
	- Also wrote spark scala API’s. Automating the jobs in Hadoop and Linux using Autosys tool.

	- Sqooping data from relational databases like Oracle to hdfs/hive and vice versa.
	- experience with hadoop ecosystem and bigdata related distributories.
	- Used Hadoop tools like
		- hdfs for storage,
		- mapreduce to process data in backend,
		- hadoop data warehouse as hive, in order to export and import data from different rdbms written the sqoop scripts.

project documentation
=====================
- There are two types of documenatation, I do.
1) Active documentation
	- It is shared with all stack holders.
	- It is needed as part of the process, for the code review, and further, for signoff, from stackholders, for code production release.
2) Pasive documentation
	- I create for project source code maintenance.
	- I write both comments and docstrings.
	- Use pydoc module, for creating documentation from these docstrings


TODO - Steganography with python
