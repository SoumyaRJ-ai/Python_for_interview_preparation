- Lambda layer
	- It is an archive containing additional code, such as libraries, dependencies, or
	  even custom runtimes.
	- When you include a layer in a function, the contents are extracted to the /opt
	  directory in the execution environment.

- AWS Lambda Performance Optimizaton
	- increasing RAM => faster execution => same price.
	- Watch out for function size to reduce the cold start durations.
	- Split complex processes into separate functions to save money and gain speed.
	- When possible, execute code in parallel.
	- Reusing connections with Keep-Alive.

- AWS Lambda Advantages
	- Reduced Cost of Execution
	- Improved Application Resiliency

- AWS Lambda Limitations
	- The disk space (ephemeral) is limited to 512 MB.
	- The default deployment package size is 50 MB.
	- The memory range is from 128 to 3008 MB.
	- The maximum execution timeout for a function is 15 minutes*.
	- Requests limitations by Lambda:
		- Request and response (synchronous calls) body payload size can be up to to 6 MB.
		- Event request (asynchronous calls) body can be up to 128 KB.
	- More Complex Call Patterns
	- No Control Over Environment

- AWS Lambda event source mappings
	- An event source mapping is a Lambda resource that reads from an event source and
	invokes a Lambda function.
	- we can use event source mappings to process items from a stream or queue in services
	that don't invoke Lambda functions directly.
	- Lambda provides event source mappings for
		- Amazon DynamoDB
		- Amazon Kinesis
		- Amazon MQ
		- Amazon Managed Streaming for Apache Kafka (Amazon MSK)
		- Self-managed Apache Kafka


Amazon Simple Queue Service (Amazon SQS)
- AWS Lambda Examples
	- say, for Returning a message

	def lambda_handler(event, context):
		message = 'Hello {} {}!'.format(event['first_name'], event['last_name'])
		return {
			'message' : message
		}

	- The first argument is the event object.
	  An event is a JSON-formatted document that contains data for a Lambda function
	  to process. The Lambda runtime converts the event to an object and passes it to
	  your function code. It is usually of the Python dict type. It can also be list,
	  str, int, float, or the NoneType type.
	- The event object contains information from the invoking service.
	  When you invoke a function, you determine the structure and contents of the event.
	  When an AWS service invokes your function, the service defines the event structure.
	- The second argument is the context object.
	  A context object is passed to your function by Lambda at runtime.
	  This object provides methods and properties that provide information about the
	  invocation, function, and runtime environment.

	- AWS lambda can be invoked both synchronously or asynchronously
	  In synchronous invocation, lambda runs the function and waits for response.
		 when function completes, lambda returns response from the function's code with
		 additional data, such as the version of the function that was invoked.

		AWS CLI command for the same:
			aws lambda invoke --function-name my-function --payload '{ "key": "value" }' response.json

	  In asynchronous invocation, we dont wait for function response.
		you handoff event to lambda and handles rest.
		we configure how Lambda handles errors, and can send invocation records to a downstream
		resource to chain together components of your application.
		AWS services like S3 and SNS invoke functions asynchronously to process the events.
		Error handling for asynchronous events:
		we can configure
			1) Maximum age of event – The maximum amount of time Lambda retains an event
							 in the asynchronous event queue, up to 6 hours.
			2) Retry attempts – The number of times Lambda retries when the function
							 returns an error, between 0 and 2

		AWS CLI command:
			aws lambda invoke --function-name my-function  \
				  --invocation-type Event --cli-binary-format raw-in-base64-out \
						  --payload '{ "key": "value" }' response.json

- AWS Lambda Vs Glue
	- Lambda can use a number of different languages (Node.js, Python, Go, Java, etc.)
	  whereas Glue can only execute jobs using Scala or Python code.
	- Lambda can execute code from triggers by other services (SQS, Kafka, DynamoDB,
	  Kinesis, CloudWatch, etc.) vs. Glue which can be triggered by lambda events,
	  another Glue jobs, manually or from a schedule.
	- Lambda runs much faster for smaller tasks vs. Glue jobs which take longer to
	  initialize due to the fact that it's using distributed processing. That being
	  said, Glue leverages its parallel processing to run large workloads faster than Lambda.
	- Lambda looks to require more complexity/code to integrate into data sources
	  (Redshift, RDS, S3, DBs running on ECS instances, DynamoDB, etc.) while Glue
	  can easily integrate with these. However, with the addition of Step Functions,
	  multiple lambda functions can be written and ordered sequentially due reduce
	  complexity and improve modularity where each function could integrate into a
	  aws service (Redshift, RDS, S3, DBs running on ECS instances, DynamoDB, etc.)
	- Glue looks to have a number of additional components, such as Data Catalog which
	  is a central metadata repository to view your data, a flexible scheduler that
	  handles dependency resolution/job monitoring/retries, AWS Glue DataBrew for
	  cleaning and normalizing data with a visual interface, AWS Glue Elastic Views
	  for combining and replicating data across multiple data stores, AWS Glue Schema
	  Registry to validate streaming data schema.

- AWS Glue
	- https://docs.aws.amazon.com/glue/latest/dg/add-job.html
	- https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html
	- AWS Glue connectors
		- Amazon S3
		- JDBC
			- Amazon Redshift
			- Amazon Relational Database Service (Amazon RDS)
		- Amazon DocumentDB
		- DynamoDB
		- Kafka
		- Amazon Kinesis
		- MongoDB
		- Network (designates a connection to a data source within an
					Amazon Virtual Private Cloud environment (Amazon VPC))

- ETL Jobs
	- I have created ETLs jobs like say,
		Users can connect to an endpoint, from where request goes via
			AWS Congnito for authentication , Then
			AWS API Gateway and then
			aws lambda and from there
			it can make AWS Athena queries on top of s3 buckets
			and reply that response in JSON

- dead letter queue
	- A dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can
	target for messages that can't be delivered to subscribers successfully.
	- Messages that can't be delivered due to client errors or server errors are held
	in the dead-letter queue for further analysis or reprocessing.

AWS account security tools
===========================

- AWS IAM (Identity & Access Management)
	- Enables to create users and roles with permissions
	  too specific resources
	- Supports Multi-factor authentication & supports
	  single sign-on (SSO)

- Amazon GuardDuty
	- uses machine learning to look for malicious activity in AWS environment
	- combines cloudTrail event logs, VPC flow logs, S3 event logs
	  and DNS logs to continuouslt monitor and analyze all activity.
	- identifies issues like privilege escalation, exposed credentials, and
	  communication with malicious IP addresses and domains.
	- detects when Ec2 instances are serving malware or mining bitcoin

	- Also, detects access pattern anomalies such as API calls
	  in new regions, ...

- Amazon Macie
	- discovers and protects sensitive data stored in s3 buckets.
	- like personally-identifiable information, or personal health information,
	  through discovery jobs.
	- continously evaluates s3 buckets and alerts when bucket is
	  unencrypted, is publicly accessible, or is shared with aws accounts
	  outside the organization.

- AWS config
	- records and continuously evaluates your AWS resource configuration.
	- keeping a historical record of all changes to your resources, which is useful for compliance with legal requirements and your organization’s policies.
	- evaluates new and existing resources against rules that validate certain configurations.
	- Config is configured per region, so it’s essential to enable AWS Config in all regions to ensure all resources are recorded, including in regions where you don’t expect to create resources.

- AWS CloudTrail
	- tracks all activity in AWS environment.
	- It records all actions a user executes in the AWS console and all API calls as events.
	- You can view and search these events to identify unexpected or unusual requests in your AWS environment.
	- AWS CloudTrail Insights is an add-on to help identify unusual activity. It automatically analyzes your events and raises an event when it detects abnormal activity.
	- CloudTrail is enabled by default in all AWS accounts

- Security Hub
	- combines information from all the above services in a central, unified view. It collects data from all security services from multiple AWS accounts and regions, making it easier to get a complete view of your AWS security posture.


Application Security Tools
===========================
- Amazon Inspector
	- security assessment service for applications deployed on EC2.
	- assessments include network access, common vulnerabilities and exposures (CVEs), Center for Internet Security (CIS) benchmarks, and common best practices such as disabling root login for SSH and validating system directory permissions on your EC2 instances.
	- Based on agent application installed in EC2 VMs, Inspector
	  generates a report with a detailed list of security findings prioritized by severity.
	- Run Inspector as part of a gated check in deployment pipeline to assess your applications’ security before deploying to production.

- AWS Shield
	- fully-managed distributed denial-of-service (DDoS) protection service.
	- enabled by default as a free standard service with protection against common DDoS attacks against your AWS environment.

- AWS Web Application Firewall (WAF)
	- monitors and protects applications and APIs built on services such as CloudFront, API Gateway, and AppSync.
	- can block access to your endpoints based on different criteria such as the source IP address, the request’s origin country, values in headers and bodies, and more (i.e, you can enable rate limiting, only allowing a certain number of requests per IP

- AWS Secrets Manager
	- managed service where you can store and retrieve sensitive information such as database credentials, certificates, and tokens.
	- Use fine-grained permissions to specify exact actions an entity can perform on the secrets, such as creating, updating, deleting, or retrieving secrets.
	- Secrets Manager also supports automatic rotation for AWS services such as Amazon Relational Database Service (RDS).


Amazon Athena Workgroups are resource types, used to separate query execution and query
history between Users, Teams, or Applications running under the same AWS account.
Because Workgroups act as resources, you can use resource-based policies to control access
to a Workgroup.

i have experience migrating applications and data, from on-prem to cloud

AWS IAM
========
	- IAM roles define the set of permissions for making AWS service request
	- IAM policies define the permissions that you will require.


	- Admin or root user for the account can create IAM identities.
	- An IAM identity provides access to an AWS account.
	- user group is a collection of IAM users managed as a unit.
	- IAM identity represents a user, and can be authenticated and then authorized to
	  perform actions in AWS.
	  Each IAM identity can be associated with one or more policies.

	- Policies determine what actions a user, role, or member of a user group can perform,
	  on which AWS resources, and under what conditions.
		- Identity-based policies are permissions policies that you attach to an IAM identity,
		  such as an IAM user, group, or role.
		- Resource-based policies are permissions policies that you attach to a resource such
		  as an Amazon S3 bucket or an IAM role trust policy.

RDS vs DynamoDb
================
	-  Amazon RDS is relational, whereas DynamoDB is a NoSQL database engine.
	- DynamoDB can support tables of any size, where as in RDS, the storage size changes
	 based on the database engine used.
	- Storing data in DyanmoDb costs more, compared to RDS or Aurora.
	- DynamoDb is very fast as it is key-value database

	- DynamoDb should not be used:
		- When multi-item or cross table transactions are required.
		- When complex queries and joins are required.
		- When real-time analytics on historic data is required.

SNS vs SQS
==========
	SNS - sends messages to the subscriber using push mechanism and no need of pull.
	SQS - it is a message queue service used by distributed applications to exchange
	      messages through a polling model, and can be used to decouple sending and
		  receiving components.


AWS Limits
==========
	Lambda Environment variables limit is not per function; but overall @ no more than 4KB
		   disk space (ephemeral) is limited to 512 MB
		   default deployment package size is 50 MB
		   memory range is from 128 to 3008 MB
		   maximum execution timeout for a function is 15 minutes
		   3000 concurrent request at time. If exceeded, requsts will throttle until lambda scales by 500 per minute


Cloud Formation
===============
- intrinsic function
	- built-in functions that enable you to assign values to properties that are only available
	  at runtime. 
	- we can use intrinsic functions in templates, to assign values to properties that are not 
	  available until runtime.
	- Example 
		Fn::Base64
		Fn::Cidr
		Condition functions
		Fn::FindInMap
		Fn::GetAtt
		Fn::GetAZs
		Fn::ImportValue
		Fn::Join
		Fn::Select
		Fn::Split
		Fn::Sub
		Fn::Transform


boto3
=======
- boto3 is built on top of botocore
	- client is for lower-level interactions
	- resource for higher-level object oriented abstractions
	- waiters are polling the status of specific resource
		- say, when creating ec2 instance, waiting till it reaches "Running" state
	- collections indicate a group of resources such as a group of s3 objects
	  in a bucket, or a group of SQS queues
		- It helps to perform some action on those groups

- List files in s3 bucket
	import boto3
	s3 = boto3.resource('s3')

	## Bucket to use
	bucket = s3.Bucket('my-bucket')

	## List objects within a given prefix
	for obj in bucket.objects.filter(Delimiter='/', Prefix='fruit/'):
		print(obj.key)

	Output:

		fruit/apple.txt
		fruit/banana.txt
