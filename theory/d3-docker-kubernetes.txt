Docker
======
- Use cases
	- Simplifying configuration
	- App isolation
	- server consolidation
	- multi-tenancy
	- code pipeline management
	- Rapid Deployment
	- Developer productivity
	- Debugging capabilities

- docker Commands
----------
1. `docker ps`  # current containers
2. `docker run` # create and start the container
		-it 			flags provide an interface for us to type commands into.
		--name 			flag is used to give a nice, memorable name to the container.
						You can name it anything, such as "i-love-potato".
		--network=host 	setting gives the container access to your computer's network.
						This is very useful when writing MicroPython code that interacts
						with your home network or with the internet.
		--mount 		flag allows us to share a specific folder on our computer with
						the container.
						In this case, I am mounting my folder named Code into the
						container's /home folder, so that I can run my code from there.
		--entrypoint 	flag specifies which command to run when the container starts.
						I want to run the bash shell right away so that I can type in commands.

3. `docker create` # create container
4. `dokcer exec` # to run commnads in container for once
5. `docker stop [container ID]` # terminate the container and save it's state by commit it
6. `docker rm [container ID]` # remove container
7. `docker inspect [container ID]` # Get more info about running container
___
7. `docker images` # list the images
8. `docker push` # push your image to docker repo
9. `docker pull` # download an image from docker repo
10. `docker commit` # create an image from container
11. `docker rmi` # remove image
___
12. `docker volume` # create a docker volume
13. `docker network` # create a docker network
14. `docker build` # build a new image from dockerfile

		docker build -t imageName:imageTag pathToDockerfileDirectory/

		Example:
		docker build -t vieux/apache:2.0
15. docker checkpoint create  - creates a new checkpoint
16. docker checkpoint ls  - lists existing checkpoints
17. docker checkpoint rm  - deletes an existing checkpoint

- docker best practices
------------------------
1) image size
	- smaller images consume less disk space, can be downloaded(& extracted) faster,
	  and thus reduce the container start time.

2) Use .dockerignore files
	- It is similar to the git ignore files.
	- To avoid accidentally copying unnecessary files into the image.


3) SQUASH Image layers
	- docker-squash is a Python-based tool that squashes the last N layers of an image to a single layer.
	- It is similar to the git squash.
	- After squashing the affected layers, the deleted intermediate files and folders are not part of the squashed layer anymore.

4) Choosing a suitable base image
	- alphine images are very size-optimized(~3MB), by default.
	  But as alphine uses musl instead of much common glibc C-library, sometimes
	  compatibility issues will araise
	- Else, will try choosing linux images with slim tag
	  (e.g. from ~40 MB to ~20 MB in case of Debian).
	- But, if we choose security focused linux images, like distroless, it will be
		- harder to debug running containers, and
		- having a rather steep learning curve to customise them

5) Multi-stage builds
	- we can split the build process to two(or more) separate images
		1) A "build image" into which we install all packages and compilers, and do the compilation
			- it will contain extra things like
				- the package manager, which is needed to install packages, but not needed for running application
				- cache that package manager downloaded dependencies, which are not need for running application
				- tools which are needed for compiling binaries after installation, but not needed afterwards.
		2) A "run image" into which we only copy application code and other compiled libraries,
		  which we copy from "build image" to "run image".

6) consolidating the RUN commands
	- Actually, each RUN command adds a new layer to the image, which contains the changed files and folders.
	- These layers are “additive”, using an overlay-type file system.
	- However, deleting files only marks these files as deleted – no disk space is reclaimed!
	- Instead, we are combine all the run commands with two amperscents (&&) symbols.
	- Eg:
		INSTEAD OF
			FROM debian:latest
			WORKDIR /app
			RUN git clone https://some.project.git
			RUN cd project
			RUN make
			RUN mv ./binary /usr/bin/
			RUN cd .. && rm -rf project

		THIS CAN BE USED
			FROM debian:latest
			WORKDIR /app
			RUN git clone https://some.project.git && \
			  cd project && \
			  make && \
			  mv ./binary /usr/bin/ && \
			  cd .. && rm -rf project

7) Save space when installing dependencies
	- Disable (or clean) the cache of the package manager
	- We can instruct the package manager to install, as fewer additional dependencies as possible, explicitly
		- like avoid weak dependency links, etc
	For Debian/Ubuntu:
		apt-get install -y --no-install-recommends <list of package names to install> && <optional: do something with packages> && apt-get clean && rm -rf /var/lib/apt/lists/*
		This ensures that no recommended packages are installed, and that the cache is cleared at the end.
	For RPM-based systems, like RHEL:
		dnf -y install --setopt=install_weak_deps=False <list of package names to install> && <optional: do something with packages> && dnf clean all
		The “dnf clean all” command ensures that all caches are deleted.
	For Python (pip):
		pip install --no-cache-dir <list of package names to install>
		The --no-cache-dir argument ensures that no cache is created to begin with.
	For Node.js (npm):
		npm ci && npm cache clean --force
		The “npm ci” command (docs) is more efficient and clean than “npm install“. The second command cleans out the NPM cache.

8) Avoid superfluous chowns
	- with chowns, if some statements in dockerfile modifies a file in build
	container, in anyway, a whole new copy of that file is stored in new layer.
	- So, instead of, say,

			COPY code .
			RUN chown -R youruser code

	  you should do

			COPY --chown=youruser code .
	- This will perform the chown as part of the COPY, ensuring that only one instance of the files is created.


9) Use the docker-slim tool
	- docker-slim is a tool that reduces the image size, by starting a temporary container of the image,
	and figuring out (via static + dynamic analyses) which files are really used by  application in that container.
	- docker-slim then builds a new single-layer image that contains only those files and folders that were actually used.
	- Advantages
		- results in smaller images; sometimes, even smaller than alphine images
		- High image security, because all tools like shell, curl, package managers, etc, are removed, unless
		  application uses them.
		- Also, helps dealing with many (unoptimized) RUN statements
	- Disadvantages
		- sometimes applications will be lazy loading/calling.
			Then docker-slim will throw many of the files, which it thinks as unused.
			Say our application works in multi-language, and we have translation files.
			Those files will be used, only when some work comes in that language.
			docker-slim thinks they are not used and deletes.

	- To handle this disadvantage, we need to tweak it via
		- Explicit "preserve path" lists, that contain path to files and folders that are definitely needed.
		- Dynamic probes, which are HTTP requests or shell command calls that docker-slim makes to application,
		  to force it to lazy load dependencies, ex: native libraries.


- docker.yaml
--------------

	FROM python:3.9

	# Create app directory
	WORKDIR /app

	# Install app dependencies
	COPY src/requirements.txt ./

	RUN pip install -r requirements.txt

	# Bundle app source
	COPY src /app

	EXPOSE 8080
	CMD [ "python", "server.py" ]

- flask docker.yaml file

	FROM python:3.8-slim-buster

	WORKDIR /python-docker

	COPY requirements.txt requirements.txt
	RUN pip3 install -r requirements.txt

	COPY . .

	CMD [ "python3", "-m" , "flask", "run", "--host=0.0.0.0"]


- runnning docker container
	$ cd python-docker
	$ docker build -t python-docker-dev .
	$ docker run --rm -it -p 8080:8080 python-docker-dev


Ref: https://hasura.io/blog/how-to-write-dockerfiles-for-python-web-apps-6d173842ae1d/

- docker compose commands
==========================
docker-compose up 	# create and start containers
docker-compose up -d # create and start containers, in detached mode
docker-compose down 	# stop and remove containers, networks, images and volumes
docker-compose logs		# view output from containers
docker-compose restart	# Restart all services
docker-compose pull 	# Pull all image services
docker-compose build	# Build all image services
docker-compose config 	# validate and view the composite file
docker-compose scale <service_name>=<replica> 	# Scale special services
docker-compose top		# Display the running processes
docker-compose run -rm -p 2022:22 web bash	# start web service and runs bash as its command, remove old container.

- docker-service
================
docker service create <options> <image> <command> # create new service
docker service inspect --pretty <service_name>		# Display detailed information services
docker service ls 									# List services
docker service ps 									# List the tasks of services
docker service scale <service_name>=<replica>		# Scale special services
docker service update <service_name>=<replica>		# Update special services

- docker stack
===============
docker stack ls 									# List all running applications on this docker host
docker stack deploy -c <compositefile> <appname>	# Run the specified compose file
docker stack services <appname>						# List the services associated with an app
docker stack ps <appname>							# List the running containers associated with an app
docker stack rm <appname>							# Tear down an application

-docker machine
===============
docker-machine create --driver virtualbox myvm1			# Create a VM (Mac, Win7, Linux)
docker-machine create -d hyperv --hyperv-virtual- switch "myswitch" myvm1 	# Win10
docker-machine env myvm1								# View basic information about your node
docker-machine ssh myvm1 "docker node ls"				# List the nodes in your swarm
docker-machine ssh myvm1 "docker node inspect <nodeID>" # Inspect a node
docker-machine ssh myvm1 "docker swarm join-token -q worker"  # View join token
docker-machine ssh myvm1								# Open an SSH session with the VM; type "exit" to end
docker-machine ssh myvm2 "docker swarm leave"			# Make the worker leave the swarm
docker-machine ssh myvm1 "docker swarm leave -f"		# Make master leave, kill swarm
docker-machine start myvm1								# Start a VM that is currently not running
docker-machine stop $(docker-machine ls					# Stop all running VMs
docker-machine rm $(docker-machine ls -q)				# Delete all VMs and their disk images
docker-machine scp docker-compose.yml myvm1:~ 			# Copy file to node's home dir
docker-machine ssh myvm1 'docker stack deploy -c <file> <app>"	# Deploy an app

- Kubernetes
=============
- We used EKS in AWS
- Elastic kubernets service
- AWS has three: ECS, Fargate and EKS
- We used EKs as it is cloud agnostic
- Docker for creating containers

Pods
	- They are the smallest unit of deployment & can contain more than 1 containers.

Namespace
	- Its an abstraction that allows to divide a cluster into multiple scoped "Virtual Clusters".

Deployment
	- Object that can contain applications and run multiple replicas.

Services
	- Abstraction to expose an app as a service(like microservice), Load balancing traffic.

Persistent Volume (PV)
	- A storage volume that can be attached to pod.
	- can be created either by administrator( static provisioning) or create on the fly(dynamic provisioning) via storage class.

Persistent Volume Claims(PVC)
	- A request for storage by a user. PVC consumes PV resource.


-  Ingress
	- An object that allows access to your Kubernetes services from outside the Kubernetes cluster.
	- we can configure access by creating a collection of rules that define which inbound connections reach which services.
	- Ingress traffic is composed of all the data communications and network traffic originating from external networks and destined for a node in the host network. Ingress traffic can be any form of traffic whose source lies in an external network and whose destination resides inside the host network.

- TODO: https://head-first-kubernetes.github.io/

- secrets vs configMaps
	- Secrets  are used for storing secret like API keys, credentials, etc
	- ConfigMaps are used for storing not-secret configuration data

- Ingress usecases
	Providing externally reachable URLs for services.
	Load balancing traffic.
	Offering name-based virtual hosting.
	Terminating SSL (secure sockets layer) or TLS (transport layer security)

- Ingress vs Load balancing
	- load balancer distributes the requests among multiple backend services (of same type)
	  whereas ingress is more like an API gateway (reverse proxy) which routes the request
	  to a specific backend service based on, for instance, the URL.

- Deployment Strategies
	1) Rolling deployment
		—replaces pods running the old version of the application with the new version,
		one by one, without downtime to the cluster.
	2) Recreate
		—terminates all the pods and replaces them with the new version.
	3) Ramped slow rollout
		—rolls out replicas of the new version, while in parallel, shutting down old replicas.
	4) Best-effort controlled rollout—specifies
		- a “max unavailable” parameter which indicates what percentage of existing pods can
		be unavailable during the upgrade, enabling the rollout to happen much more quickly.
	5) Canary deployment
		— uses a progressive delivery approach, with one version of the application serving
		most users, and another, newer version serving a small pool of test users.
		- The test deployment is rolled out to more users if it is successful.

Terraform commands
	refresh -> to get the current state of infrastructure
	Plan    -> create an execution plan
	apply   -> execute the plan
	destroy -> destroy the resources/infrastructure

Helm
	I have experience working with Helm package manager for creating and managing Kubernetes clusters
	I Used Helm charts to automate the deployment of my project kubernetes clusters, and dependencies.
	I have first hand experience in writing the Helm YAML files.
	Also, used Helm CLI commands for installing the charts and deploying infra.

Heroku
	# deploy to Heroku
	git push heroku main

	# Procfile
	web: mercury run 0.0.0.0:$PORT

TODO - https://github.com/sunnybhambhani/kubernetes



=========================

Being a go developer
	build pipeline -> linting - compiling -> dockerImage -> PR Merged -> image to ECR
		- feature, pull request against master, trigger pipeline
			fail fast
		- compilation
		- artifact is jar/dockerfile
	Deployment Pipeline ->

docker image size control
multi-build docker file
docker - ADD vs COPY, ENTRYPOINT vs CMD
securing docker file
Aquasec -- for analyzing docker images

	slim, alphine, full, distroless(), headless --- check this classfication

Helm Charts - multiple files
	- to control app


kubernetes deployment two ways - kubctl,

components
	server side
		1) kube api server
		2) etcd server
		3) controllers
		4) Scheduler

	worker side
		kubelet (agent to report)


route53 -> ELB/ALB -> Kubernetes pods

kubernetes.yaml
helmchart.yaml it is like make ; can be applied for mulltiple environments; app rollback

rpm vs yum
	- rpm will not manage dependency. We ned to give all
	- yum will automatically get all dependencies

Kubectl vs helmchart

Tools
	- AWS : ECS, fargate(managed), EKS
		- EKS
	- Metrics server
	- secrets operator - syncing secrets from external to within
	- certmanager operator
	- externalDNS operator

OpenShift
*** kubeadm - managing entire infrastructure, and provisioning
