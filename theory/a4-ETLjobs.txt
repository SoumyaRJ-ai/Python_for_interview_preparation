ETL Jobs
=========
I have created ETL jobs with event-driven architecture
worked with multiple ETL jobs.

1) s3 bucket -- > new file comes --> triggered lambda --> pandas dataframes process --> push to dB --

2) time triggered  ---> periodically, it will check for new data in the files and dump them together
		say it may run for every 1 hr
3) http triggered -- > some one want to push data to out application
		--- I exposed my lambda via API gateway --

4) aws kinesis stream ---> SQS ---> event triggered ---> AWS lambda

5) MYSQL db ->  aws kinesis stream -> aws lambda → MYSQL db NEW

6) S3 bucket →  athena queries  → AWS lambda— > AWS API gateway –JSON-> powerBI

Also, used the AWS Glue jobs for transformation of around 50k records.
I configured the glue data catalogue to pick the specific columns AND  perform operations
	Within the python code, i connected with the pyspark for some transformations, etc
And dumped final results in either parquet for any further jobs, or to the db

s3 -> AWS Athena -> QuickSight

Funnel is the ETL tool, to pull the social media data, to a csv
GCP DataFusion

ETL best practices
===================
1) Extract necessary data only
	- Data profiling and cleansing, to remove duplicate and unnecessary data.

1) Using Logging and Monitoring; we can store details like
	- The timing of each data extraction
	- The time for each extraction
	- The number of rows inserted, changed, or deleted during each extraction
	- The transcripts of any system or validation errors
2) Use Pre-Built Integrations for Seamless Pipeline Building
	- as most ETL tools like Informatica, AWS glue, ... has prebuilt integrations
	  that move data from source to destination, will little or no code.
3) Choose the Right ETL Tool for You; selection of specific tool, depends on
	- Data sources and targets/destinations, in the project
	- Business initiatives, the data integration pipeline, need to serve.
	- THen, ease of use, or superior monitoring and logging capabilities to help resolve performance issues faster.
	- Also,
		- ETL tool cost, and value add to the business.
		- Pricing model - some offer by number of data connectors, others with data volume.
		- Customer Service/support


Data Migration Strategy
------------------------
We can take offsets from the db and transfer,
	Say when I pulled there were 100 records
	Say after active migration, we repoint to new db, so, no more data to old db.

	Say now there were 120 records count
	Remaining small chunk (20 records), we will get after active migration

	I know, i will make hold of offset when pull the data
	I mean, i will count the no of records at that time, when i am pulling

Slowly Changing Dimensions
---------------------------
- dimensions that change slowly over time, rather than changing on regular schedule, time-base.
- implementing one of the SCD types should enable users assigning proper dimension's attribute
  value for given date.
- Example of such dimensions could be: customer, geography, employee.
- SCD approaches can be like
	Type 0 - The passive method
	Type 1 - Overwriting the old value
	Type 2 - Creating a new additional record
	Type 3 - Adding a new column
	Type 4 - Using historical table
	Type 6 - Combine approaches of types 1,2,3 (1+2+3=6)
- In detail,
	Type 0
		- The passive method.
		- In this method no special action is performed upon dimensional changes.
		- Some dimension data can remain the same as it was first time inserted, others may
		  be overwritten.
	Type 1
		- Overwriting the old value.
		- In this method no history of dimension changes is kept in the database.
		- The old dimension value is simply overwritten be the new one.
		- This type is easy to maintain and is often use for data which changes are caused
		  by processing corrections(e.g. removal special characters, correcting spelling errors).
	Type 2
		- Creating a new additional record.
		- In this methodology all history of dimension changes is kept in the database.
		- You capture attribute change by adding a new row with a new surrogate key to the
		  dimension table.
		- Both the prior and new rows contain as attributes the natural key(or other durable
          identifier).
		- Also 'effective date' and 'current indicator' columns are used in this method.
		- There could be only one record with current indicator set to 'Y'.
		- For 'effective date' columns, i.e. start_date and end_date, the end_date for current
		  record usually is set to value 9999-12-31.
		- Introducing changes to the dimensional model in type 2 could be very expensive
		  database operation so it is not recommended to use it in dimensions where a new
		  attribute could be added in the future.
	Type 3
		- Adding a new column.
		- In this type usually only the current and previous value of dimension is kept in
		  the database.
		- The new value is loaded into 'current/new' column and the old one into 'old/previous'
		  column.
		- Generally speaking the history is limited to the number of column created for
		  storing historical data.
		- This is the least commonly needed technique.
	Type 4
		- Using historical table.
		- In this method a separate historical table is used to track all dimension's
		  attribute historical changes for each of the dimension.
		- The 'main' dimension table keeps only the current data e.g. customer and
		  customer_history tables.
	Type 6
		- Combine approaches of types 1,2,3 (1+2+3=6).
		- In this type we have in dimension table such additional columns as:

		current_type 	- for keeping current value of the attribute. All history records
                          for given item of attribute have the same current value.
		historical_type - for keeping historical value of the attribute. All history
		                  records for given item of attribute could have different values.
		start_date 		- for keeping start date of 'effective date' of attribute's history.
		end_date 		- for keeping end date of 'effective date' of attribute's history.
		current_flag 	- for keeping information about the most recent record.

		- In this method to capture attribute change we add a new record as in type 2.
		- The current_type information is overwritten with the new one as in type 1.
		- We store the history in a historical_column as in type 3.


Data Quality Checks
	- is there numeric data
	-
