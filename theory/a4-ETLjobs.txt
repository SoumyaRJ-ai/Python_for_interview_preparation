ETL Jobs
=========
I have the experience in creating ETL jobs, like

1) In a s3 bucket, when a new file comes, that event will trigger a lambda, which parses that csv file content, and transforms using pandas dataframes, and finally push the resultant information, into AWS RDS, using RDS Proxy.

2) Implemented a time-triggered ETL pipeline that periodically loads new data files:
	- First, stored the raw data files in S3 buckets
	- Then, used AWS Cloudwatch Events to trigger an AWS Lambda function, every 1 hour
	- The Lambda function, written in python, will
		- Get the list of files in the s3 bucket using boto3
		- filtered new files added in last 1 hour, based on LastModified date
		- Loaded all files, using pandas, cleansed, transformed and appended all dataframes, into single dataframes
		- Dumped the final dataframe into AWS Redshift using psycopg2 library
		- Handled errors and retries gracefully in the Lambda function.
		- Used AWS CloudWatch Logs to monitor the Lambda and set alarms for errors.
		- Lastly, scheduled the CloudWatch Event to trigger every 1 hour based on a fixed rate expression.


3) http triggered -- > some one want to push data to out application
		--- I exposed my lambda via API gateway --

4) MYSQL db ->  aws kinesis stream -> aws lambda → MYSQL db NEW

5) S3 bucket →  athena queries  → AWS lambda— > AWS API gateway –JSON-> powerBI

6) aws kinesis stream ---> SQS ---> event triggered ---> AWS lambda

Also, used the AWS Glue jobs for transformation of around 50k records.
I configured the glue data catalogue to pick the specific columns AND  perform operations
	Within the python code, I connected with the pyspark for some transformations, etc
And dumped final results in either parquet for any further jobs, or to the db

Funnel is the ETL tool, to pull the social media data, to a csv
GCP DataFusion

ETL Job :: aws kinesis stream ---> SQS ---> event triggered ---> AWS lambda
=================================================================
	Step 1: Data is ingested into an Amazon Kinesis stream.
		Data can be ingested into a Kinesis stream from a variety of sources, including:

			a) Application logs:
				- Kinesis can be used to collect application logs from web servers, databases, and other applications.
			b) Sensor data:
				- Kinesis can be used to collect sensor data from devices such as thermostats, security cameras, and fitness trackers.
			c) Financial data:
				- Kinesis can be used to collect financial data from stock tickers, market data feeds, and other sources.

	Step 2: The data is then passed to an Amazon SQS queue.
		Once the data is in a Kinesis stream, it can be passed to an SQS queue. SQS is a message queuing service that makes it easy to decouple applications and components.

		For example, you could have an application that generates application logs. The application could write the logs to a Kinesis stream. Then, you could have another application that reads the logs from the Kinesis stream and passes them to an SQS queue. The second application could be a batch processing application that runs every night. This way, the first application can generate the logs as they are generated, and the second application can process the logs in batches at a time that is convenient for it.

	Step 3: When a message arrives in the SQS queue, it triggers an AWS Lambda function.
		When a message arrives in an SQS queue, it can trigger an AWS Lambda function. Lambda is a serverless computing platform that allows you to run code without provisioning or managing servers.

		For example, you could have a Lambda function that reads messages from an SQS queue and writes them to a database. The Lambda function could also be used to perform data transformations, such as converting data from one format to another.

	Step 4: The Lambda function can then process the data and take any necessary action.
		The Lambda function can take any action that you need it to take.
		For example, it could:

			Write the data to a database.
			Send an email notification.
			Trigger another AWS service.


AWS ETL job types
------------------
	1) Lambda-triggered ETL
		- Trigger ETL workflows using AWS Lambda whenever a file arrives in S3 or a record is added to DynamoDB/Kinesis. Provides serverless automation.

	2) 	Batch ETL
		- Extract data from sources like S3, transform it, and load into data warehouses like Redshift for analytics.
		- Scheduling tools like Airflow can automate and orchestrate these batches.

	3) Streaming ETL
		- Continuously extract streaming data from sources like Kinesis, process events in real-time, and load into destinations like Elasticsearch.
		- Useful for real-time analytics and data pipelines.

	4) Metadata-driven ETL
		- Use AWS Glue crawlers to scan data sources, catalog metadata like schemas and tables, and then automate ETL jobs based on the catalog.

	5) Containerized ETL
		- Build Docker containers with your ETL logic and run them on ECS or EKS.
		- Helps standardize and deploy ETL processing.

	6) Machine Learning ETL
		- Enrich data during ETL by passing it through ML models for classification, predictions, recommendations etc. before loading.
	7) Database migration
		- Tools like AWS Database Migration Service can help move databases from on-prem RDBMS or warehouses into cloud data stores.

ETL best practices
------------------------
1) Extract necessary data only
	- Data profiling and cleansing, to remove duplicate and unnecessary data.

1) Using Logging and Monitoring; we can store details like
	- The timing of each data extraction
	- The time for each extraction
	- The number of rows inserted, changed, or deleted during each extraction
	- The transcripts of any system or validation errors
2) Use Pre-Built Integrations for Seamless Pipeline Building
	- as most ETL tools like Informatica, AWS glue, ... has prebuilt integrations
	  that move data from source to destination, will little or no code.
3) Choose the Right ETL Tool for You; selection of specific tool, depends on
	- Data sources and targets/destinations, in the project
	- Business initiatives, the data integration pipeline, need to serve.
	- THen, ease of use, or superior monitoring and logging capabilities to help resolve performance issues faster.
	- Also,
		- ETL tool cost, and value add to the business.
		- Pricing model - some offer by number of data connectors, others with data volume.
		- Customer Service/support


Data Pipeline
--------------
	Let me quickly explain the recent data pipeline, which we have built.

	The data source is the AWS s3. The spark inside the AWS data bricks extracts the data from AWS s3 and creates the data frame and does the actual data cleansing, transformations, deduplication, and standardization.
	Then the final data will be pushed to the AWS redshift data warehouse.

	We have used airflow to orchestrate the data pipelines. Here I was involved in building the DAG and tasks using the operators that airflow provides and dependencies between the tasks. The monitor will be pretty good in airflow. We can see the tree view, graph view, and
	logs for each task independently so that we can debug things quickly if anything goes wrong. And also, we can configure the alerting framework.
	so that it will send alerts for each failure.

	For this entire pipeline, we have integrated CICD using Jenkins. Jenkins job builds the egg files with the code base and uploads these egg files into
	data bricks file system and create the data bricks jobs in the data bricks using notebook tasks and these egg dependencies.

	Let me quickly explain the working environment. I was working in the agile environment, where we used to have sprint planning meetings, scrum calls, and retro meetings for every sprint. The sprint duration was 2 weeks. We used JIRA for the project management and Github for the version control


Databricks Experience
======================
	1) ETL Pipelines
		- Used Spark DataFrames APIs in Python to transform large datasets stored in S3, Redshift and DynamoDB
		- Built data pipelines to extract, transform and load data into Delta Lake for analytics
		- Leveraged schema inference, handling null and duplicate values during transformations

			Extract data(read data from sources like S3, Redshift, DynamoDB, etc)
					users_df = spark.read.parquet('s3://bucket/users/')
					orders_df = spark.read.format('jdbc').options(...).load()

			Transform data (transformations like filtering, aggregations, handling nulls)
					clean_users_df = users_df.filter(users_df.age > 0)
					orders_df = orders_df.groupby('user_id').agg({'amount':'sum'})
					joined_df = users_df.join(orders_df, 'user_id', 'left')

			Load to Delta Lake
					clean_users_df.write.format('delta').mode('overwrite').save('/delta/users')
					joined_df.write.format('delta').partitionBy('country').mode('append').save('/delta/users_orders')


	2) Machine Learning
		- Trained ML models like XGBoost, logistic regression on structured data using Spark MLlib in Python
		- Built pipelines for feature engineering, hyperparameter tuning and model evaluation
		- Deployed models and exposed predictions using Spark MLflow

	3) Stream Processing
		- Used Structured Streaming in Python to consume and process real-time data streams
		- Applied windowing aggregations, joins and filters on event streams
		- Built dashboards on stream analytics using PySpark, Plotly and Flask

	4) Orchestration
		- Created interactive notebooks to orchestrate Spark jobs, schedule workflows using cron schedules and handle failures
		- Automated pipelines with Job Bookmarks and Delta Live Tables for idempotent execution

	5) Monitoring & Optimization
		- Enabled adaptive query execution, caching, broadcast joins to optimize Spark workloads
		- Used Spark UI and logs to monitor job performance, resource usage and tuning

normal apache spark vs databricks
---------------------------------
	Architecture
		Apache Spark runs on Hadoop YARN or Mesos cluster managers
		Databricks runs on its own proprietary runtime optimized for low latency and high concurrency

	Performance
		Databricks runtime provides higher performance - optimized JVM, caching, indexing
		Better support for iterative workloads with spark optimized file format

	Scalability
		Databricks scales seamlessly to 100s of nodes without throughput degradation
		Apache Spark needs tuning configuration, pool sizing for large clusters

	High Availability
		Databricks offers zero downtime high availability with hot standby nodes
		Apache Spark needs additional components like Zookeeper for HA

	Security
		Databricks has inbuilt Role Based Access Control, auditing capabilities
		Apache Spark relies on Hadoop/OS level security

	Cost
		Apache Spark is open source and can be self-hosted
		Databricks charges based on usage and number of nodes

	Workloads
		Apache Spark is very flexible and can run a wide variety of workloads
		Databricks is optimized for data engineering and machine learning use cases


Data Migration Strategy
------------------------
We can take offsets from the db and transfer,
	Say when I pulled there were 100 records
	Say after active migration, we repoint to new db, so, no more data to old db.

	Say now there were 120 records count
	Remaining small chunk (20 records), we will get after active migration

	I know, I will make hold of offset when pull the data
	I mean, I will count the no of records at that time, when I am pulling
